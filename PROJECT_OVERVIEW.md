# CMPE 257 Project: Stock Price Prediction Pipeline - Detailed Overview

## Introduction
This project is a comprehensive machine learning pipeline for predicting stock prices using historical data. It focuses on time-series forecasting with sliding window features, walk-forward cross-validation, and multiple model comparisons. The goal is to demonstrate end-to-end ML engineering, from data processing to model deployment.

**Key Features**:
- Data processing with sliding windows (2-5 days) for feature engineering.
- Walk-forward cross-validation to simulate real-time prediction.
- Multiple models: Baselines (Linear, Ridge) to advanced (XGBoost, LSTM with GPU support).
- Automation via scripts, Makefile, and config-driven training.
- Evaluation with metrics, plots, and reports.
- Production-ready code with documentation and troubleshooting.

**Tech Stack**: Python, scikit-learn, PyTorch, XGBoost, pandas, matplotlib, YAML configs.

## Project Structure
```
cmpe_257_project/
├── configs/                 # Configuration files for experiments
├── data/                    # Data directories (raw, processed, normalized)
├── models/                  # Model implementations
├── reports/                 # Generated reports and plots
├── results/                 # Training results and metrics
├── scripts/                 # Utility scripts for processing/training
├── src/                     # Core training and utility code
├── artifacts/               # Temporary files (e.g., metrics JSONs)
├── experiments/             # (Legacy) Old experiment scripts
├── GPU_INSTRUCTIONS.md      # Guide for GPU setup
├── Makefile                 # Automation shortcuts
├── README.md                # Main documentation
├── requirements.txt         # Python dependencies
├── run_all_linear.sh        # Shell script for batch training
└── .gitignore               # Git ignore rules
```

## Detailed File Explanations

### Configuration Files (`configs/`)
- **`xgb_lstm.yaml`**: YAML config for XGBoost and LSTM experiments. Defines models with parameters (e.g., `n_estimators` for XGBoost, `hidden_size` for LSTM), windows (2-5 days), metrics (MSE, MAE, R²), and options like `save_models`. Used by `src/main.py` for reproducible training.

### Data Directories (`data/`)
- **`raw/`**: Contains original Kaggle CSV files (e.g., `prices.csv`, `fundamentals.csv`). These are the unprocessed datasets downloaded from Kaggle.
- **`naive_processed/`**: Simple processed data without walk-forward validation. Used for quick baselines (generated by `scripts/process_data_naive.py`).
- **`full_processed/`**: Fully processed data with walk-forward splits. Contains train/test/eval sets for each window/fold (e.g., `X_train_window_2_fold_0.csv`). Generated by `scripts/process_data_full.py`.
- **`normalized/`**: Normalized data ready for training. Includes scalers (e.g., `scaler_pipeline.pkl`) fitted on training data. Generated by `scripts/build_pipeline.py`.

### Model Files (`models/`)
- **`__init__.py`**: Model registry. Imports and registers all models (e.g., `linear_regression`, `xgboost_regressor`, `lstm_regressor`). Uses lazy loading for optional deps like XGBoost.
- **`lstm.py`**: Implements `LSTMRegressor`, a PyTorch-based model for sequence prediction. Handles data reshaping, GPU detection, training loops, and evaluation. Configurable for hidden layers, dropout, epochs, etc.

### Report and Results (`reports/`, `results/`)
- **`reports/model_report.md`**: Auto-generated markdown report with model comparisons and insights (from `scripts/report_plots.py`).
- **`reports/figs/`**: Directory for generated plots (e.g., RMSE comparisons).
- **`results/`**: Stores training outputs. Subdirs like `xgboost_regressor/` contain `results_summary.csv`, `results.csv`, and `model_config.yaml` for each experiment.

### Scripts (`scripts/`)
- **`initial_data_exploration.py`**: Explores raw CSV files. Prints column info, dtypes, missing values, and basic stats to understand the data.
- **`process_data_full.py`**: Main data processor. Loads raw data, creates sliding windows, applies walk-forward validation (5 folds), and saves to `data/full_processed/`. Handles date splits for train/eval.
- **`process_data_naive.py`**: Quick processor for simple baselines. Creates basic windows without full validation.
- **`full_data_exploration.py`**: Analyzes processed data. Generates stats, correlations, and visualizations for normalized datasets.
- **`build_pipeline.py`**: Normalization pipeline. Fits StandardScalers on training data per fold and normalizes all sets, saving to `data/normalized/`.
- **`train_linear.py`**: Trains baseline models (Ridge/LinearRegression). Takes window/fold args, saves models, metrics, and plots.
- **`train_tree_nn.py`**: Trains advanced baselines (RF, GBR, MLP). Similar to above but for tree/neural models.
- **`aggregate_metrics.py`**: Combines JSON metrics from runs into a single CSV (`reports/metrics_summary.csv`).
- **`report_plots.py`**: Generates comparison plots and markdown reports from aggregated metrics.
- **`grid_search.py`**: Hyperparameter tuner. Runs grid searches on specified models/windows/folds and logs best params.

### Core Code (`src/`)
- **`main.py`**: Orchestrator for config-driven training. Loads YAML configs, runs cross-validation for each model, and saves results.
- **`train.py`**: Handles cross-validation logic. Loads data, fits models, computes metrics, and returns results DataFrames.
- **`utils.py`**: Utility functions for loading configs, saving results, and summarizing metrics.

### Automation and Helpers
- **`Makefile`**: Provides shortcuts (e.g., `make normalize` runs `scripts/build_pipeline.py`). Simplifies common tasks.
- **`run_all_linear.sh`**: Batch script to run `train_linear.py` across all windows/folds. Automates baseline training.
- **`requirements.txt`**: Lists pinned dependencies (e.g., `numpy==1.26.4` to avoid conflicts). Install with `pip install -r requirements.txt`.
- **`.gitignore`**: Excludes generated files (e.g., `results/`, `models/*.pkl`, `.DS_Store`) to keep the repo clean.

### Documentation
- **`README.md`**: Main guide. Covers setup, pipeline steps, usage, and troubleshooting. Essential for onboarding.
- **`GPU_INSTRUCTIONS.md`**: Step-by-step for GPU testing on Colab/cloud. Includes code for CUDA detection.

## How to Get Started
1. **Clone and Setup**: `git clone <repo> && cd cmpe_257_project && pip install -r requirements.txt`.
2. **Process Data**: Run `python scripts/process_data_full.py` then `python scripts/build_pipeline.py`.
3. **Train Models**: Use `python -m src.main --config configs/xgb_lstm.yaml` or scripts like `run_all_linear.sh`.
4. **Analyze**: Run `python scripts/aggregate_metrics.py && python scripts/report_plots.py`.
5. **Contribute**: Follow `GIT_WORKFLOW.md` for changes.

## Key Workflows
- **Full Pipeline**: `make full` (processes data, trains baselines, aggregates results).
- **Model Comparison**: Train with configs, review `results/` and `reports/`.
- **GPU Testing**: Follow `GPU_INSTRUCTIONS.md` for Colab.
- **Tuning**: Use `scripts/grid_search.py` for optimization.

This project emphasizes reproducibility, scalability, and real-world ML practices. For questions, check the README or open an issue!